{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "616c91ff-d807-4add-8cc6-2b7134cda396",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import utils\n",
    "import graphing\n",
    "\n",
    "SEED = 15243\n",
    "np.random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "plt.rc(\"axes.spines\", right=False, top=False)\n",
    "plt.rc(\"font\", family=\"serif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131abc8b",
   "metadata": {},
   "source": [
    "## Importing the dataset\n",
    "The below code is used to import the dataset from the the folder at the path `DATA_DIR`. The dataset is then loaded using `Keras`'s `image_dataset_from_directory` function.\n",
    "\n",
    "### Note\n",
    "The dataset is not uploaded to the notebook as it is too large. The dataset can be downloaded from [here](https://www.kaggle.com/andrewmvd/face-mask-detection) and uploaded to the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7b42fc6-5df0-4a66-a408-28dd994ae6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aaa78b-9ef5-4f6c-a42f-fb321c0f30ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 25\n",
    "\n",
    "os.listdir(DATA_DIR)\n",
    "IMG_SIZE = 224\n",
    "MAX_EPOCHS = 50\n",
    "class_names = [\"normal\", \"polyps\", \"ulcerative-colitis\"]\n",
    "class_details = [\"Normal\", \"Polyps\", \"Ulcerative Colitis\"]\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    DATA_DIR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    class_names=class_names,\n",
    "    seed=45,\n",
    "    subset=\"training\",\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "validation_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    DATA_DIR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    class_names=class_names,\n",
    "    seed=45,\n",
    "    subset=\"validation\", \n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b54b12",
   "metadata": {},
   "source": [
    "## Previewing the dataset\n",
    "The dataset is then previewed by displaying the first 9 images from the dataset along with their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0ec310-da3c-4544-9673-80721f9af9f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 7))\n",
    "for images, labels in train_ds.take(1):\n",
    "    labels = labels.numpy()\n",
    "    for i in range(8):\n",
    "        ax = plt.subplot(2, 4, i + 1)\n",
    "        x = images[i].numpy().astype(\"uint8\")\n",
    "        plt.imshow(x, cmap='gist_gray', vmin=0, vmax=255)\n",
    "        plt.title(class_details[labels[i]])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "11bb314b-4d1a-4706-a6fa-f9f2fa2d727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "validation_ds = validation_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33458d82",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "The pretrained base models used are:\n",
    "1. VGG16\n",
    "2. EfficientNetB0\n",
    "3. EfficientNetB1\n",
    "4. EfficientNetV2B0\n",
    "5. EfficientNetV2S\n",
    "6. MobileNetV3Small\n",
    "\n",
    "The models are defined below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b82c70-c4ad-49c6-b52a-5b71b00001b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG-16 Model\n",
    "pretrained_vgg16_base = tf.keras.applications.vgg16.VGG16(\n",
    "    include_top=False, weights=\"imagenet\", pooling=None\n",
    ")\n",
    "pretrained_vgg16_base.trainable = False\n",
    "\n",
    "vgg16_model = tf.keras.Sequential([\n",
    "    layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
    "    layers.Lambda(tf.keras.applications.vgg16.preprocess_input),\n",
    "    pretrained_vgg16_base,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(128, activation=\"relu\"),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(3, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "vgg16_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2744b9c-3b6a-44d6-a8d9-aefbc62575ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EfficientNet B0 Model\n",
    "\n",
    "pretrained_efficientnetB0_base = tf.keras.applications.efficientnet.EfficientNetB0(\n",
    "    include_top=False, weights=\"imagenet\", pooling=None,\n",
    ")\n",
    "pretrained_efficientnetB0_base.trainable = False\n",
    "\n",
    "efficientnetB0_model = tf.keras.Sequential([\n",
    "    layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
    "    pretrained_efficientnetB0_base,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(128, activation=\"relu\"),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(3, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "efficientnetB0_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d4050f-222f-4d5c-b9ad-ff0db547f2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EfficientNet B1 Model\n",
    "\n",
    "pretrained_efficientnetB1_base = tf.keras.applications.efficientnet.EfficientNetB1(\n",
    "    include_top=False, weights=\"imagenet\", pooling=None,\n",
    ")\n",
    "pretrained_efficientnetB1_base.trainable = False\n",
    "\n",
    "efficientnetB1_model = tf.keras.Sequential([\n",
    "    layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
    "    pretrained_efficientnetB1_base,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(128, activation=\"relu\"),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(3, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "efficientnetB1_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3c432a-d147-4659-82d3-c38f5ebb7a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EfficientNet V2-B0\n",
    "\n",
    "pretrained_efficientnetV2B0_base = tf.keras.applications.EfficientNetV2B0(\n",
    "    include_top=False, weights=\"imagenet\", pooling=None,\n",
    ")\n",
    "pretrained_efficientnetV2B0_base.trainable = False\n",
    "\n",
    "efficientnetV2B0_model = tf.keras.Sequential([\n",
    "    layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
    "    layers.Lambda(tf.keras.applications.efficientnet_v2.preprocess_input),\n",
    "    pretrained_efficientnetV2B0_base,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    # layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(128, activation=\"relu\"),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(3, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "efficientnetV2B0_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe8b6d7-5d6c-40fb-a055-983de20295e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EfficientNet V2S Model\n",
    "\n",
    "pretrained_efficientnetV2S_base = tf.keras.applications.EfficientNetV2S(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    pooling=None,\n",
    "    include_preprocessing=True\n",
    ")\n",
    "\n",
    "pretrained_efficientnetV2S_base.trainable = False\n",
    "\n",
    "efficientnetV2S_model = tf.keras.Sequential([\n",
    "    layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
    "    pretrained_efficientnetV2S_base,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(128, activation=\"relu\"),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(3, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "efficientnetV2S_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a35c35-ec5c-4f19-a032-308105b23026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobileNet V3S\n",
    "pretrained_mobilenetV3S_base = tf.keras.applications.MobileNetV3Small(\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    pooling=None\n",
    ")\n",
    "\n",
    "pretrained_mobilenetV3S_base.trainable = False\n",
    "\n",
    "mobilenetV3S_model = tf.keras.Sequential([\n",
    "    layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
    "    layers.Lambda(tf.keras.applications.mobilenet_v3.preprocess_input),\n",
    "    pretrained_mobilenetV3S_base,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    layers.Dense(3, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "for layer in pretrained_mobilenetV3S_base.layers[-30:]:\n",
    "    if not isinstance(layer, layers.BatchNormalization):\n",
    "        layer.trainable = True\n",
    "        \n",
    "mobilenetV3S_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4702a51d",
   "metadata": {},
   "source": [
    "## Training the models\n",
    "\n",
    "We consolidate the training of the models below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de07a70f-b773-4e5a-b7f5-797ffd54aecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models in a list of list of dictionaries\n",
    "models = [\n",
    "    {\n",
    "        \"name\": \"VGG-16\",\n",
    "        \"id\": \"vgg16\",\n",
    "        \"model\": vgg16_model,\n",
    "        \"custom_objects\": {\"preprocess_input\": tf.keras.applications.vgg16.preprocess_input},\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"EfficientNet B0\",\n",
    "        \"id\": \"efficientnetB0\",\n",
    "        \"model\": efficientnetB0_model,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"EfficientNet B1\",\n",
    "        \"id\": \"efficientnetB1\",\n",
    "        \"model\": efficientnetB1_model,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"EfficientNet V2-B0\",\n",
    "        \"id\": \"efficientnetV2B0\",\n",
    "        \"model\": efficientnetV2B0_model,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"EfficientNet V2S\",\n",
    "        \"id\": \"efficientnetV2S\",\n",
    "        \"model\": efficientnetV2S_model,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"MobileNet V3S\",\n",
    "        \"id\": \"mobilenetV3S\",\n",
    "        \"model\": mobilenetV3S_model,\n",
    "        \"custom_objects\": {\"preprocess_input\": tf.keras.applications.mobilenet_v3.preprocess_input},\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7531c8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the above models to a new list\n",
    "trained_models = models.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2990c7de-07df-4043-ad70-6329e720308b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the models, saving their history, figures, and TensorFlow checkpoints\n",
    "checkpoint_path = f\"./models/{MAX_EPOCHS}e\"\n",
    "for i, model in enumerate(models):\n",
    "    print(f\"Starting training for model #{i + 1}: {model['name']}\")\n",
    "    model_history = utils.compile_and_fit_model(model['model'], train_ds=train_ds, validation_ds=validation_ds, epochs=MAX_EPOCHS, early_stopping=False, patience=2, checkpoint_path=f\"{checkpoint_path}/{model['id']}-{IMG_SIZE}.keras\")\n",
    "    trained_models[i]['history'] = model_history\n",
    "    print(f\"Saving figures for model #{i + 1}: {model['name']}\")\n",
    "    utils.save_figure_from_history(model_history, f\"{checkpoint_path}/{model['id']}-{IMG_SIZE}.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fef00da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a confusion heatmap for the MobileNet V3S model\n",
    "graphing.confusion_heatmap(*(utils.get_predictions(mobilenetV3S_model, ds=validation_ds)), class_details, percentage=False, path=\"./mobilenet-heatmap.svg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
